{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import pandas as pd\n",
    "\n",
    "#An implementation of a neural network. It is possible to use GridSearch from scikit-learn to optimize hyperparameters, \n",
    "#for example the number of hidden layers, and the learning rate. This implementation is based on Andrew NG's online lectures\n",
    "#on neural networks, https://class.coursera.org/ml-003/lecture, accessed in June 2016.\n",
    "class neural_network(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self,number_of_hidden_layers = 2,learning_rate = 0.7,initial_weight_limit = 0.7,tuning_parameter = 0,iterations = 1200,derivative_check = False):\n",
    "        #Set the number of iterations\n",
    "        self.iterations = iterations\n",
    "        #Set the learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "        #Set the tuning parameter\n",
    "        self.tuning_parameter = tuning_parameter\n",
    "        #Set the number of hidden layers. Minimum number is 1\n",
    "        self.number_of_hidden_layers = number_of_hidden_layers\n",
    "        #Set the range for the initial value of the weights\n",
    "        self.initial_weight_limit = initial_weight_limit\n",
    "        #Set the derivative_check variable\n",
    "        self.derivative_check = derivative_check\n",
    "        #If the derivative check is True, calculate numerical derivatives\n",
    "        if self.derivative_check == True:\n",
    "        #Initialize the numerical derivative list. we will use it to check if the \n",
    "        #calculation of the derivatives is correct.\n",
    "            self.numerical_derivative = []\n",
    "            self.weights_output_derivative_test = []\n",
    "            #Set the epsilon value that will be used to calculate the numerical derivative        \n",
    "            self.epsilon = 0.00001\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"iterations\": self.iterations, \"learning_rate\": self.learning_rate,\"tuning_parameter\": self.tuning_parameter,\"number_of_hidden_layers\": self.number_of_hidden_layers,\"initial_weight_limit\": self.initial_weight_limit}\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self,parameter, value)\n",
    "        return self\n",
    "    def fit(self,X,y):\n",
    "        self.error = []\n",
    "        y = np.reshape(y,(len(y),1))\n",
    "        intercept = np.ones(len(X))\n",
    "        #Scale the data to have mean 0 and standard deviation 1\n",
    "        self.mean = np.array(np.mean(X,axis = 0))\n",
    "        self.standard_deviation = np.std(X, axis = 0)\n",
    "        X = preprocessing.scale(X)\n",
    "        X = np.c_[X,intercept]\n",
    "        self.data = np.array(X)\n",
    "        self.target = np.array(y)\n",
    "        #Set the number of nodes in the hidden layers. \n",
    "        #The number of nodes in layers excludes the bias term\n",
    "        self.nodes_in_hidden_layers = np.shape(X)[1]//2\n",
    "        #Start the weight matrices. The dimensions of the first and last weights are different\n",
    "        #then the middle weights   \n",
    "        np.random.seed(1)\n",
    "        self.weights_first = 2 * np.random.uniform(size = (np.shape(self.data)[1],self.nodes_in_hidden_layers)) * self.initial_weight_limit - self.initial_weight_limit #np.ones((np.shape(self.data)[1],self.nodes_in_hidden_layers))       \n",
    "        self.weights_first_derivative = np.ones((np.shape(self.data)[1],self.nodes_in_hidden_layers))        \n",
    "        if self.number_of_hidden_layers > 1:         \n",
    "            self.weights_middle =  2 * np.random.uniform(size = (self.number_of_hidden_layers - 1,self.nodes_in_hidden_layers + 1,self.nodes_in_hidden_layers)) * self.initial_weight_limit - self.initial_weight_limit #np.ones((self.number_of_hidden_layers - 1,self.nodes_in_hidden_layers + 1,self.nodes_in_hidden_layers))       \n",
    "            self.weights_middle_derivative = np.ones((self.number_of_hidden_layers - 1,self.nodes_in_hidden_layers + 1,self.nodes_in_hidden_layers))       \n",
    "        #The last weight maps the last hidden layer to one probability score    \n",
    "        self.weights_output =  2 * np.random.uniform(size = (self.nodes_in_hidden_layers + 1,1)) * self.initial_weight_limit - self.initial_weight_limit # np.ones((self.nodes_in_hidden_layers + 1,1))\n",
    "        self.weights_output_derivative = np.ones((self.nodes_in_hidden_layers + 1,1))\n",
    "        #Hidden layer(s). The first index is for the hidden layer number, the second is the length of the dataset,\n",
    "        #and the third is the nodes in the hidden layer, plus 1 for the bias term        \n",
    "        self.hidden_layers = np.zeros((self.number_of_hidden_layers,len(self.data),self.nodes_in_hidden_layers + 1))        \n",
    "        #Set the bias terms to 1            \n",
    "        self.hidden_layers[:,:,self.nodes_in_hidden_layers] = 1\n",
    "        #Initiate the probabilities. These are the outputs of the neural network        \n",
    "        self.probabilities = np.zeros((len(self.data),1))\n",
    "        #Create the matrices that will be used for backpropagation\n",
    "        self.delta_output = np.zeros((len(self.data),1))\n",
    "        self.delta_middle = np.zeros((self.number_of_hidden_layers,len(self.data),self.nodes_in_hidden_layers))\n",
    "        for i in range(self.iterations):\n",
    "            #Start with forward propagation to calculate probabilities            \n",
    "            self.hidden_layers[0,:,:self.nodes_in_hidden_layers] = 1/(1 + np.exp(-np.dot(self.data,self.weights_first))) \n",
    "            if self.number_of_hidden_layers > 1:\n",
    "                for h in range(self.number_of_hidden_layers - 1):\n",
    "                    self.hidden_layers[h + 1,:,:self.nodes_in_hidden_layers] = 1/(1 + np.exp(-np.dot(self.hidden_layers[h,:,:],self.weights_middle[h])))\n",
    "            self.probabilities = 1/(1 + np.exp(-np.dot(self.hidden_layers[self.number_of_hidden_layers - 1],self.weights_output)))\n",
    "            #Calculate the error (J(theta))           \n",
    "            self.error.append(np.average(np.dot(self.hidden_layers[self.number_of_hidden_layers - 1],self.weights_output) * (1 - self.target) + np.log(1 + np.exp(-np.dot(self.hidden_layers[self.number_of_hidden_layers - 1],self.weights_output)))))             \n",
    "            #Back propagation            \n",
    "            #Calculate delta for the output layer:\n",
    "            self.delta_output = self.probabilities - self.target \n",
    "            derivative = self.hidden_layers[self.number_of_hidden_layers - 1,:,:self.nodes_in_hidden_layers] * (1 - self.hidden_layers[self.number_of_hidden_layers - 1,:,:self.nodes_in_hidden_layers])\n",
    "            self.delta_middle[self.number_of_hidden_layers - 1] = np.dot(self.delta_output,np.transpose(self.weights_output[:self.nodes_in_hidden_layers])) * derivative\n",
    "            if self.number_of_hidden_layers > 1:\n",
    "                for h in range(self.number_of_hidden_layers - 1):\n",
    "                    derivative = self.hidden_layers[self.number_of_hidden_layers - h - 2,:,:self.nodes_in_hidden_layers] * (1 - self.hidden_layers[self.number_of_hidden_layers - h - 2,:,:self.nodes_in_hidden_layers])\n",
    "                    self.delta_middle[self.number_of_hidden_layers - h - 2] = np.dot(self.delta_middle[self.number_of_hidden_layers - h - 1],np.transpose(self.weights_middle[self.number_of_hidden_layers - h - 2,:self.nodes_in_hidden_layers])) * derivative\n",
    "            #Calculate derivative of the cost function with respect to each weight  \n",
    "            self.weights_output_derivative = np.dot(np.transpose(self.hidden_layers[self.number_of_hidden_layers - 1]),self.delta_output)/len(self.data)          \n",
    "            #Regularization for weights except for the weight for the bias term\n",
    "            self.weights_output_derivative[:self.nodes_in_hidden_layers] = self.weights_output_derivative[:self.nodes_in_hidden_layers] + self.tuning_parameter * self.weights_output_derivative[:self.nodes_in_hidden_layers]           \n",
    "            if self.derivative_check == True:\n",
    "            ##Check if the derivatives are calculated correctly##            \n",
    "            #We can check if the derivatives are calculated correctly, by comparing\n",
    "            #them with the derivatives calculated by numerical estimation.\n",
    "            #Use 2-sided difference to calculate the derivatives of the first weights\n",
    "                weight_index = 1\n",
    "                self.weights_output_upper = np.copy(self.weights_output)\n",
    "                self.weights_output_upper[weight_index] = self.weights_output_upper[weight_index] + self.epsilon\n",
    "                error_upper = np.average(np.dot(self.hidden_layers[self.number_of_hidden_layers - 1],self.weights_output_upper) * (1 - self.target) + np.log(1 + np.exp(-np.dot(self.hidden_layers[self.number_of_hidden_layers - 1],self.weights_output_upper))))\n",
    "                self.weights_output_lower = np.copy(self.weights_output)\n",
    "                self.weights_output_lower[weight_index] = self.weights_output_lower[weight_index] - self.epsilon            \n",
    "                error_lower = np.average(np.dot(self.hidden_layers[self.number_of_hidden_layers - 1],self.weights_output_lower) * (1 - self.target) + np.log(1 + np.exp(-np.dot(self.hidden_layers[self.number_of_hidden_layers - 1],self.weights_output_lower))))            \n",
    "                self.numerical_derivative.append((error_upper - error_lower)/ (2 * self.epsilon))            \n",
    "                self.weights_output_derivative_test.append(self.weights_output_derivative[weight_index][0])            \n",
    "            if self.number_of_hidden_layers > 1:\n",
    "                for h in range(self.number_of_hidden_layers - 1):\n",
    "                    self.weights_middle_derivative[self.number_of_hidden_layers - h - 2] = np.dot(np.transpose(self.hidden_layers[self.number_of_hidden_layers - h - 2]),self.delta_middle[self.number_of_hidden_layers - h - 1])/len(self.data)\n",
    "                    #Regularization for weights except for the weight for the bias term\n",
    "                    self.weights_middle_derivative[self.number_of_hidden_layers - h - 2,:self.nodes_in_hidden_layers,:] = self.weights_middle_derivative[self.number_of_hidden_layers - h - 2,:self.nodes_in_hidden_layers,:] + self.tuning_parameter * self.weights_middle_derivative[self.number_of_hidden_layers - h - 2,:self.nodes_in_hidden_layers,:]      \n",
    "            self.weights_first_derivative = np.dot(np.transpose(self.data),self.delta_middle[0])/len(self.data)\n",
    "            #Regularization for weights except for the weight for the bias term\n",
    "            self.weights_first_derivative[:self.nodes_in_hidden_layers] = self.weights_first_derivative[:self.nodes_in_hidden_layers] + self.tuning_parameter * self.weights_first_derivative[:self.nodes_in_hidden_layers]\n",
    "            #Update the parameters using the derivatives calculated by backpropagation\n",
    "            self.weights_first = self.weights_first - self.learning_rate * self.weights_first_derivative\n",
    "            if self.number_of_hidden_layers > 1:\n",
    "                self.weights_middle = self.weights_middle - self.learning_rate * self.weights_middle_derivative\n",
    "            self.weights_output = self.weights_output - self.learning_rate * self.weights_output_derivative            \n",
    "        if self.derivative_check == True:\n",
    "            print(pd.DataFrame({'numerical':self.numerical_derivative,'analytic':self.weights_output_derivative_test}))\n",
    "    def predict_proba(self,newdata):\n",
    "        newdata = preprocessing.scale(newdata)\n",
    "        intercept = np.ones(len(newdata))\n",
    "        newdata = np.c_[newdata,intercept]        \n",
    "        #Hidden layer(s)        \n",
    "        self.hidden_layers = np.zeros((self.number_of_hidden_layers,len(newdata),self.nodes_in_hidden_layers + 1))        \n",
    "        #Set the bias terms to 1            \n",
    "        self.hidden_layers[:,:,self.nodes_in_hidden_layers] = 1\n",
    "        #Probabilities        \n",
    "        self.probabilities_test = np.zeros((len(newdata),1))\n",
    "        self.hidden_layers[0,:,:self.nodes_in_hidden_layers] = 1/(1 + np.exp(-np.dot(newdata,self.weights_first))) \n",
    "        if self.number_of_hidden_layers > 1:\n",
    "            for h in range(self.number_of_hidden_layers - 1):\n",
    "                self.hidden_layers[h + 1,:,:self.nodes_in_hidden_layers] = 1/(1 + np.exp(-np.dot(self.hidden_layers[h,:,:],self.weights_middle[h])))\n",
    "        self.probabilities_test = 1/(1 + np.exp(-np.dot(self.hidden_layers[self.number_of_hidden_layers - 1],self.weights_output)))\n",
    "        self.probabilities_test = np.ravel(self.probabilities_test)\n",
    "        return self.probabilities_test        \n",
    "    def predict(self,newdata):\n",
    "        newdata = preprocessing.scale(newdata)\n",
    "        intercept = np.ones(len(newdata))\n",
    "        newdata = np.c_[newdata,intercept]        \n",
    "        #Hidden layer(s)        \n",
    "        self.hidden_layers = np.zeros((self.number_of_hidden_layers,len(newdata),self.nodes_in_hidden_layers + 1))        \n",
    "        #Set the bias terms to 1            \n",
    "        self.hidden_layers[:,:,self.nodes_in_hidden_layers] = 1\n",
    "        #Start probabilities array        \n",
    "        self.probabilities_test = np.zeros((len(newdata),1))\n",
    "        #Start predictions array\n",
    "        self.predictions_test = np.zeros((len(newdata),1))\n",
    "        self.hidden_layers[0,:,:self.nodes_in_hidden_layers] = 1/(1 + np.exp(-np.dot(newdata,self.weights_first))) \n",
    "        if self.number_of_hidden_layers > 1:\n",
    "            for h in range(self.number_of_hidden_layers - 1):\n",
    "                self.hidden_layers[h + 1,:,:self.nodes_in_hidden_layers] = 1/(1 + np.exp(-np.dot(self.hidden_layers[h,:,:],self.weights_middle[h])))\n",
    "        self.probabilities_test = 1/(1 + np.exp(-np.dot(self.hidden_layers[self.number_of_hidden_layers - 1],self.weights_output)))\n",
    "        self.predictions_test = [0 if x < 0.5 else 1 for x in self.probabilities_test]\n",
    "        return self.predictions_test\n",
    "    def get_error_list(self):\n",
    "    #Get the errors from each iteration\n",
    "        return self.error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
