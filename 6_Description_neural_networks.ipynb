{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes a neural network for binary classification. The neural network fitting algorithm finds the weights which are used to map the data into successive layers to calculate a probability for each observation.\n",
    "\n",
    "$n$: number of observations.\n",
    "\n",
    "$k$: number of variables.\n",
    "\n",
    "$H$: number of hidden layers.\n",
    "\n",
    "$X$: $n$ by $k$ data matrix.\n",
    "\n",
    "$y$: $n$ by 1 array of binary outputs.\n",
    "\n",
    "$\\hat{y}$: $n$ by 1 array of probability prediction of the neural networks.\n",
    "\n",
    "$h$: index for the hidden layer\n",
    "$m$: number of nodes in each hidden layer, excluding the bias term.\n",
    "\n",
    "$a^{(h)}$: $h^{th}$ hidden layer, an $n$ by $m + 1$ matrix, including the bias term. \n",
    "\n",
    "$\\theta^{(1)}:$ Initial weights, a $k$ by $m + 1$ matrix that maps $X$ to the first hidden layer. The $(m + 1)^{st}$ term is the bias term.\n",
    "\n",
    "$\\theta^{(h)}:$ The $h^{th}$ layer's weigths, an $m + 1$ by $m$ matrix.\n",
    "\n",
    "$\\theta^{(H)}:$ The last layer's weigths, an $m + 1$ by $1$ matrix that maps the last layer $a^{(H)}$ to $\\hat{y}$.\n",
    "\n",
    "$g()$: Activation function used to map the previous layer to the next layer. For the binary classification task, the logistic function is used:\n",
    "\n",
    "$$g(a^{(h-1)}\\theta^{(h-1)}) = \\frac{1}{1 + e^{-a^{(h-1)}\\theta^{(h-1)}}} = a^{(h)}$$\n",
    "\n",
    "$J(\\theta)$: The cost function the neural network fitting algorithm tries to minimize. The cost function is the log-likelihood of the logistic function.\n",
    "\n",
    "Starting with the Likelihood function for a binomial variable, with probability $p$:\n",
    "\n",
    "$L(p;x) = \\prod_{i=1}^{i=n} p^{y_i}(1-p)^{1-y_i}$\n",
    "\n",
    "$ll(p;x) = \\sum_{i=1}^{i=n} y_iln(p) + (1-y_i)ln(1-p)$\n",
    "\n",
    "Substitute the logistic function for $p$.\n",
    "\n",
    "$ll(p;x)=\\sum_{i=1}^{i=n} y_iln(\\frac{1}{1 + e^{-a^{(H)}\\theta^{(H)}}}) + (1-y_i)ln(1-\\frac{1}{1 + e^{-a^{(H)}\\theta^{(H)}}})$\n",
    "\n",
    "$\\sum_{i=1}^{i=n} -y_iln(1 + e^{-a^{(H)}\\theta^{(H)}}) + (1-y_i)[{-a^{(H)}\\theta^{(H)}}-ln({1 + e^{-a^{(H)}\\theta^{(H)}}})]$\n",
    "\n",
    "$\\sum_{i=1}^{i=n} -a^{(H)}\\theta^{(H)}(1-y_i)-ln(1+e^{-a^{(H)}\\theta^{(H)}})$\n",
    "\n",
    "Fitting a logistic regression maximizes this log-likelihood, which is equivalent to minimizing the negative of the log-likelihood. The cost function then is the average of the negative of the log-likelihood function over all observations:\n",
    "\n",
    "$J(\\theta)=\\frac{1}{n}\\sum_{i=1}^{i=n} a^{(H)}\\theta^{(H)}(1-y_i)+ln(1+e^{-a^{(H)}\\theta^{(H)}})$\n",
    "\n",
    "Here is the outline of the algorithm to minimize the cost function:\n",
    "\n",
    "1) Randomy initialize weights.\n",
    "\n",
    "2) Calculate the predicted probabilities using the weigths. \n",
    "\n",
    "3) Calculate the derivative of the cost function with respect to the weights.\n",
    "\n",
    "4) Update the weights.\n",
    "\n",
    "5) Repeat 2 to 4 until the error function does not improve more than a threshold, or until the preset number of iterations are completed. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The derivative of the cost function with respect to the last set of weights (called here 'output weights') are calculated as follows: \n",
    "\n",
    "$\\frac{dJ(\\theta)}{d\\theta^{(H)}}=\\frac{1}{n}\\sum_{i=1}^{i=n}a^{(H)}(1-y_i)-\\frac{a^{(H)}e^{-a^{(H)}\\theta^{(H)}}}{1+e^{-a^{(H)}\\theta^{(H)}}}$\n",
    "\n",
    "$\\frac{dJ(\\theta)}{d\\theta^{(H)}}=\\frac{1}{n}\\sum_{i=1}^{i=n}a^{(H)}(1-y_i)-a^{(H)}(1-\\hat{p_i})$\n",
    "\n",
    "$\\frac{dJ(\\theta)}{d\\theta^{(H)}}=\\frac{1}{n}\\sum_{i=1}^{i=n}a^{(H)}(\\hat{p_i}-y_i)$\n",
    "\n",
    "\n",
    "The derivative of the cost function with respect to earlier weights are calculated as follows. Starting from the cost function:\n",
    "\n",
    "$J(\\theta)=\\frac{1}{n}\\sum_{i=1}^{i=n} a^{(H)}\\theta^{(H)}(1-y_i)+ln(1+e^{-a^{(H)}\\theta^{(H)}})$\n",
    "\n",
    "$J(\\theta)=\\frac{1}{n}\\sum_{i=1}^{i=n} g(a^{(H-1)}\\theta^{(H-1)})\\theta^{(H)}(1-y_i)+ln(1+e^{-g(a^{(H-1)}\\theta^{(H-1)})\\theta^{(H)}})$\n",
    "\n",
    "$\\frac{dJ(\\theta}{d\\theta^{H-1}}=\\frac{1}{n}\\sum_{i=1}^{i=n} \\frac{dg(a^{(H-1)}\\theta^{(H-1)})}{d\\theta^{(H-1)}}\\theta^{(H)}(1-y_i)-\\frac{e^{-g(a^{(H-1)}\\theta^{(H-1)})\\theta^{(H)}}\\theta^{(H)}a^{(H-1)}}{1+e^{-g(a^{(H-1)}\\theta^{(H-1)})\\theta^{(H)}}}\\frac{dg(a^{(H-1)}\\theta^{(H-1)})}{d\\theta^{(H-1)}}$\n",
    "\n",
    "$\\frac{dJ(\\theta}{d\\theta^{H-1}}=\\frac{1}{n}\\sum_{i=1}^{i=n}\\frac{dg(a^{(H-1)}\\theta^{(H-1)})}{d\\theta^{(H-1)}}\\theta^{H}a^{H-1}(\\hat{p}-y_i)$\n",
    "\n",
    "We can replace the derivative term with: \n",
    "\n",
    "$\\frac{dg(a^{(H-1)}\\theta^{(H-1)})}{d\\theta^{(H-1)}}=a^{H-1}(1-a^{H-1})$\n",
    "\n",
    "This comes from:\n",
    "\n",
    "$\\frac{dg(a^{(H-1)}\\theta^{(H-1)})}{d\\theta^{(H-1)}}=$\n",
    "\n",
    "$\\frac{d(\\frac{1}{1+e^{-a^{(H-1)}\\theta^{(H-1)}}})}{d\\theta^{(H-1)}}=(1+e^{-a^{(H-1)}\\theta^{(H-1)}})^{-2}e^{-a^{(H-1)}\\theta^{(H-1)}}$\n",
    "\n",
    "$\\frac{1}{1+e^{-a^{(H-1)}\\theta^{(H-1)}}}(1-\\frac{1}{1+e^{-a^{(H-1)}\\theta^{(H-1)}}})=a^{H-1}(1-a^{(H-1)})$\n",
    "\n",
    "Calculating the derivative of the cost function with respect to each weight is called backpropagation. After the derivatives of the cost function with respect to each weight are calculated, they can be adjusted with a learning rate and the weights can be updated:\n",
    "\n",
    "$\\theta^{(h)}=\\theta^{(h)}+\\lambda\\frac{dJ(\\theta^{(h)})}{\\theta^{(h)}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
